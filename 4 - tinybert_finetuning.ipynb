{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks on base TinyBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'huawei-noah/TinyBERT_General_4L_312D'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=312, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.381394 million params\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "checkpoint = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "# checkpoint = 'albert/albert-large-v2' #req bs16 othw vram, and training v slow\n",
    "# checkpoint = 'albert/albert-base-v2'\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "display(\n",
    "    checkpoint,\n",
    "    model,\n",
    ")\n",
    "\n",
    "print(\n",
    "    sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6,\n",
    "    'million params'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `tokenizer.model_max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not getattr(tokenizer, 'model_max_length', None) == 512:\n",
    "    tokenizer.model_max_length = 512\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2009, 2001, 1037, 3185, 2055, 103, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] it was a movie about [MASK]. [SEP]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 0.0395,  0.7269,  0.2387,  ...,  0.8403,  0.8712,  0.4771],\n",
       "         [-0.0463,  0.1668,  0.1976,  ..., -0.0349,  0.2082,  0.0940],\n",
       "         [-0.0659, -0.1432, -0.1890,  ...,  0.0323,  0.2478,  0.3310],\n",
       "         ...,\n",
       "         [-0.0183, -0.1747, -0.1914,  ...,  0.0553, -0.0635,  0.6340],\n",
       "         [ 0.0674,  1.1103,  0.7440,  ...,  0.3351,  0.0164,  0.2027],\n",
       "         [ 0.0752,  1.0598,  0.7429,  ...,  0.3009,  0.0412,  0.3249]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM head output shape: torch.Size([1, 9, 30522])\n",
      "www https oh previously http\n"
     ]
    }
   ],
   "source": [
    "mask_token = tokenizer.special_tokens_map['mask_token']\n",
    "text = f'It was a movie about {mask_token}.'\n",
    "\n",
    "display(\n",
    "    tokenizer(text),\n",
    "    tokenizer.decode( tokenizer(text)['input_ids'] ),\n",
    "    model(**tokenizer(text, return_tensors='pt')),\n",
    ")\n",
    "\n",
    "outputs = model(**tokenizer(text, return_tensors='pt'))\n",
    "print('LM head output shape:', outputs.logits.shape)\n",
    "\n",
    "logits = outputs['logits'] #(B, L, vocab)\n",
    "\n",
    "#mask_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "mask_id = tokenizer.vocab[mask_token]\n",
    "mask_ix = tokenizer(text).input_ids.index(mask_id)\n",
    "\n",
    "print(tokenizer.decode(\n",
    "    logits.detach().numpy()[0, mask_ix].argsort()[::-1][:5]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Fine-tune on SST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "if not getattr(tokenizer, 'model_max_length', None) == 512:\n",
    "    tokenizer.model_max_length = 512\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SST5 data\n",
    "\n",
    "SST5\n",
    "https://github.com/CS287/HW1/tree/master/data\n",
    "\n",
    "(0: very negative, 1: negative, 2: neutral, 3: positive, 4: very positive)\n",
    "\n",
    "Drop test set aot combining with train|val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 8544\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 1101\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# datasets = load_dataset('glue', 'sst2')\n",
    "# datasets = load_dataset('stanfordnlp/sentiment140', trust_remote_code=True)\n",
    "# datasets = load_dataset('stanfordnlp/sst', trust_remote_code=True)\n",
    "datasets = load_dataset('SetFit/sst5')\n",
    "datasets.pop('test')\n",
    "display(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap labels around. Want sst 0,1,2,3,4 neg:pos -> fft (4, 3, 2, 1, 0) neg:pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'sst5_labels', 'label_text', 'fft_labels'],\n",
       "        num_rows: 8544\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'sst5_labels', 'label_text', 'fft_labels'],\n",
       "        num_rows: 1101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst5_to_fft = {\n",
    "    0: 4, #v neg\n",
    "    1: 3,\n",
    "    2: 2,\n",
    "    3: 1,\n",
    "    4: 0, #v pos\n",
    "}\n",
    "datasets = datasets.map(\n",
    "    lambda batch: {'fft_labels': [sst5_to_fft[sst_score] for sst_score in batch['label']]},\n",
    "    batched=True\n",
    ").rename_column('label', 'sst5_labels')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"circuit queens wo n't learn a thing , they 'll be too busy cursing the film 's strategically placed white sheets .\",\n",
       "  \"if you ignore the cliches and concentrate on city by the sea 's interpersonal drama , it ai n't half-bad .\"],\n",
       " 'sst5_labels': [2, 2],\n",
       " 'label_text': ['neutral', 'neutral'],\n",
       " 'fft_labels': [2, 2]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'].shuffle()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of labels\n",
    "\n",
    "Class 4 relatively small. Will use class weights to boost scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fft_labels\n",
       "0    15.0\n",
       "1    27.0\n",
       "2    19.0\n",
       "3    26.0\n",
       "4    13.0\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets.set_format('pandas')\n",
    "datasets['train'][:].fft_labels.value_counts(normalize=True).mul(100).sort_index().round().pipe(display)\n",
    "datasets.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 8544\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = datasets\\\n",
    "    .map(lambda batch: tokenizer(batch['text'], truncation=True), batched=True)\\\n",
    "    .remove_columns(['text', 'label_text', 'sst5_labels'])\\\n",
    "    .rename_column('fft_labels', 'labels')\n",
    "\n",
    "if 'token_type_ids' in datasets['train'].column_names:\n",
    "    tokenized = tokenized.remove_column('token_type_ids')\n",
    "    \n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#Swap out the classifiation head for a new 5-class head.\n",
    "# Could alternatively use .from_trained(..., ignore_mismatched_sizes=True),\n",
    "# but seems undocumented as of yet.\n",
    "dbert_base = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "if False:\n",
    "    [new_preclf] = [module for name, module in dbert_base.named_modules() if name == 'pre_classifier']\n",
    "    [new_clf] = [module for name, module in dbert_base.named_modules() if name == 'classifier']\n",
    "\n",
    "    dbert_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'distilbert/distilbert-base-uncased-finetuned-sst-2-english' #, num_labels=5, ignore_mismatched_sizes=True\n",
    "    )\n",
    "    dbert_ft.pre_classifier = deepcopy(new_preclf)\n",
    "    dbert_ft.classifier = deepcopy(new_clf)\n",
    "\n",
    "    del dbert_base, new_preclf, new_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune on SST5, then data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: cuda  GPU name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device is:', device, ' GPU name:', torch.cuda.get_device_name())\n",
    "\n",
    "from model_utils import compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SST5 fine-tuning\n",
    "\n",
    "Class4 better with class balancing but rest v bad\n",
    "\n",
    "If boost class4 and keep others 1, ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Using class weights: [1. 1. 1. 1. 2.]\n",
      "10 epochs | 5340 training steps\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Prior to fine-tuning~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0        nan      0.00      0.00       165\n",
      "           1       0.26      0.99      0.41       279\n",
      "           2        nan      0.00      0.00       229\n",
      "           3        nan      0.00      0.00       289\n",
      "           4       0.30      0.04      0.08       139\n",
      "\n",
      "    accuracy                           0.26      1101\n",
      "   macro avg       0.28      0.21      0.10      1101\n",
      "weighted avg       0.27      0.26      0.11      1101\n",
      "\n",
      "  loss: 1.6086374247323156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8ecdbad215496dbd438f22438b8787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train steps:   0%|          | 0/5340 [00:00<?, ?minibatch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch  1) | trn 1.3465 | val 1.3567\n",
      "(epoch  2) | trn 1.1758 | val 1.2364\n",
      "(epoch  3) | trn 1.1061 | val 1.2055\n",
      "(epoch  4) | trn 1.0590 | val 1.2060\n",
      "(epoch  5) | trn 1.0339 | val 1.2243\n",
      "(epoch  6) | trn 0.9827 | val 1.2178\n",
      "(epoch  7) | trn 0.9558 | val 1.2215\n",
      "(epoch  8) | trn 0.9402 | val 1.2360\n",
      "(epoch  9) | trn 0.9327 | val 1.2382\n",
      "(epoch 10) | trn 0.9181 | val 1.2345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.56      0.55       165\n",
      "           1       0.48      0.62      0.54       279\n",
      "           2       0.43      0.23      0.30       229\n",
      "           3       0.51      0.43      0.46       289\n",
      "           4       0.42      0.63      0.50       139\n",
      "\n",
      "    accuracy                           0.48      1101\n",
      "   macro avg       0.48      0.49      0.47      1101\n",
      "weighted avg       0.48      0.48      0.47      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=5).to(device)\n",
    "model.config.id2label = {clas: label for clas, label in enumerate(['very positive', 'positive', 'neutral', 'negative', 'very negative'])}\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "#Class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1, 2, 3, 4]), y=tokenized['train'][:]['labels'])\n",
    "class_weights = None\n",
    "class_weights = np.array([1., 1., 1., 1., 2.])\n",
    "\n",
    "if class_weights is not None:\n",
    "    print('[!] Using class weights:', class_weights.round(1))\n",
    "\n",
    "#Data loaders\n",
    "batch_size = 16\n",
    "\n",
    "dynamic_padding_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized['train'], batch_size=batch_size, shuffle=True,\n",
    "    pin_memory=True, collate_fn=dynamic_padding_collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized['validation'], batch_size=batch_size,\n",
    "    pin_memory=True, collate_fn=dynamic_padding_collator\n",
    ")\n",
    "\n",
    "val_labels = np.concatenate([minibatch['labels'] for minibatch in val_loader])\n",
    "\n",
    "\n",
    "#LR scheduler\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "print(num_epochs, 'epochs |', num_training_steps, 'training steps')\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear', optimizer,\n",
    "    num_warmup_steps=int(round(0.1 * num_training_steps)),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "#Score before training\n",
    "preds, loss = compute_metrics(model, val_loader, device)\n",
    "print('Prior to fine-tuning'.center(85, '~'))\n",
    "print(classification_report(y_true=val_labels, y_pred=preds, zero_division=np.nan))\n",
    "print('  loss:', loss)\n",
    "\n",
    "\n",
    "#\n",
    "# Training loop\n",
    "#\n",
    "writer = SummaryWriter(log_dir='runs/tinybert_ft_sst5/' + f'bs{batch_size}-cw4')\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps), unit='minibatch', desc='train steps')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for minibatch in train_loader:\n",
    "        minibatch = {k: v.to(device) for k, v in minibatch.items()}\n",
    "        outputs = model(**minibatch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs.loss.backward()\n",
    "        torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device).float())(outputs['logits'], minibatch['labels']).backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update()\n",
    "    #/end of epoch\n",
    "\n",
    "    model.eval()\n",
    "    trn_preds, trn_loss = compute_metrics(model, train_loader, device)\n",
    "    val_preds, val_loss = compute_metrics(model, val_loader, device)\n",
    "\n",
    "    print(f'(epoch {epoch + 1:2d}) | trn {trn_loss:6.4f} | val {val_loss:6.4f}')\n",
    "\n",
    "    writer.add_scalars('loss', {'trn': trn_loss, 'val': val_loss}, global_step=epoch + 1)\n",
    "    \n",
    "    #Class k metrics\n",
    "    k = 4\n",
    "    class_k_true = val_labels==k\n",
    "    class_k_pred = val_preds==k\n",
    "    writer.add_scalars(\n",
    "        f'class {k} val metrics',\n",
    "        {#'recall': recall_score(class_k_true, class_k_pred),\n",
    "         'f1': f1_score(class_k_true, class_k_pred)\n",
    "         },\n",
    "         global_step=epoch + 1\n",
    "    )\n",
    "\n",
    "    clf_report = classification_report(val_labels, val_preds, zero_division=np.nan)\n",
    "    writer.add_text('classification_report', clf_report, global_step=epoch + 1)\n",
    "torch.save(model.state_dict(), f'finetuned/tinybert-sst5.pickle')\n",
    "\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntinybert bs16\\n              precision    recall  f1-score   support\\n           0       0.55      0.56      0.55       165\\n           1       0.48      0.62      0.54       279\\n           2       0.43      0.23      0.30       229\\n           3       0.51      0.43      0.46       289\\n           4       0.42      0.63      0.50       139\\n\\n    accuracy                           0.48      1101\\n   macro avg       0.48      0.49      0.47      1101\\nweighted avg       0.48      0.48      0.47      1101\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tinybert bs16\n",
    "              precision    recall  f1-score   support\n",
    "           0       0.55      0.56      0.55       165\n",
    "           1       0.48      0.62      0.54       279\n",
    "           2       0.43      0.23      0.30       229\n",
    "           3       0.51      0.43      0.46       289\n",
    "           4       0.42      0.63      0.50       139\n",
    "\n",
    "    accuracy                           0.48      1101\n",
    "   macro avg       0.48      0.49      0.47      1101\n",
    "weighted avg       0.48      0.48      0.47      1101\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3b9f0_row0_col0, #T_3b9f0_row1_col1, #T_3b9f0_row2_col2, #T_3b9f0_row3_col3, #T_3b9f0_row4_col4 {\n",
       "  color: #fcfbfd;\n",
       "}\n",
       "#T_3b9f0_row0_col1 {\n",
       "  color: #eae8f2;\n",
       "}\n",
       "#T_3b9f0_row0_col2 {\n",
       "  color: #491285;\n",
       "}\n",
       "#T_3b9f0_row0_col3 {\n",
       "  color: #40017e;\n",
       "}\n",
       "#T_3b9f0_row0_col4, #T_3b9f0_row1_col4, #T_3b9f0_row2_col0, #T_3b9f0_row3_col0, #T_3b9f0_row4_col1 {\n",
       "  color: #3f007d;\n",
       "}\n",
       "#T_3b9f0_row1_col0 {\n",
       "  color: #7567af;\n",
       "}\n",
       "#T_3b9f0_row1_col2 {\n",
       "  color: #53268f;\n",
       "}\n",
       "#T_3b9f0_row1_col3 {\n",
       "  color: #42057f;\n",
       "}\n",
       "#T_3b9f0_row2_col1 {\n",
       "  color: #7669af;\n",
       "}\n",
       "#T_3b9f0_row2_col3 {\n",
       "  color: #e0dfee;\n",
       "}\n",
       "#T_3b9f0_row2_col4 {\n",
       "  color: #4d1b89;\n",
       "}\n",
       "#T_3b9f0_row3_col1 {\n",
       "  color: #52238d;\n",
       "}\n",
       "#T_3b9f0_row3_col2 {\n",
       "  color: #bfc0de;\n",
       "}\n",
       "#T_3b9f0_row3_col4 {\n",
       "  color: #6e5aa8;\n",
       "}\n",
       "#T_3b9f0_row4_col0 {\n",
       "  color: #40027e;\n",
       "}\n",
       "#T_3b9f0_row4_col2 {\n",
       "  color: #430780;\n",
       "}\n",
       "#T_3b9f0_row4_col3 {\n",
       "  color: #5a3395;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3b9f0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3b9f0_level0_col0\" class=\"col_heading level0 col0\" >very positive</th>\n",
       "      <th id=\"T_3b9f0_level0_col1\" class=\"col_heading level0 col1\" >positive</th>\n",
       "      <th id=\"T_3b9f0_level0_col2\" class=\"col_heading level0 col2\" >neutral</th>\n",
       "      <th id=\"T_3b9f0_level0_col3\" class=\"col_heading level0 col3\" >negative</th>\n",
       "      <th id=\"T_3b9f0_level0_col4\" class=\"col_heading level0 col4\" >very negative</th>\n",
       "      <th id=\"T_3b9f0_level0_col5\" class=\"col_heading level0 col5\" >entropy</th>\n",
       "      <th id=\"T_3b9f0_level0_col6\" class=\"col_heading level0 col6\" >predicted_label</th>\n",
       "      <th id=\"T_3b9f0_level0_col7\" class=\"col_heading level0 col7\" >predicted_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3b9f0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3b9f0_row0_col0\" class=\"data row0 col0\" >0.517138</td>\n",
       "      <td id=\"T_3b9f0_row0_col1\" class=\"data row0 col1\" >0.435383</td>\n",
       "      <td id=\"T_3b9f0_row0_col2\" class=\"data row0 col2\" >0.035969</td>\n",
       "      <td id=\"T_3b9f0_row0_col3\" class=\"data row0 col3\" >0.007010</td>\n",
       "      <td id=\"T_3b9f0_row0_col4\" class=\"data row0 col4\" >0.004499</td>\n",
       "      <td id=\"T_3b9f0_row0_col5\" class=\"data row0 col5\" >1.272088</td>\n",
       "      <td id=\"T_3b9f0_row0_col6\" class=\"data row0 col6\" >very positive</td>\n",
       "      <td id=\"T_3b9f0_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b9f0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3b9f0_row1_col0\" class=\"data row1 col0\" >0.215806</td>\n",
       "      <td id=\"T_3b9f0_row1_col1\" class=\"data row1 col1\" >0.678828</td>\n",
       "      <td id=\"T_3b9f0_row1_col2\" class=\"data row1 col2\" >0.087168</td>\n",
       "      <td id=\"T_3b9f0_row1_col3\" class=\"data row1 col3\" >0.014746</td>\n",
       "      <td id=\"T_3b9f0_row1_col4\" class=\"data row1 col4\" >0.003452</td>\n",
       "      <td id=\"T_3b9f0_row1_col5\" class=\"data row1 col5\" >1.281563</td>\n",
       "      <td id=\"T_3b9f0_row1_col6\" class=\"data row1 col6\" >positive</td>\n",
       "      <td id=\"T_3b9f0_row1_col7\" class=\"data row1 col7\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b9f0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3b9f0_row2_col0\" class=\"data row2 col0\" >0.015028</td>\n",
       "      <td id=\"T_3b9f0_row2_col1\" class=\"data row2 col1\" >0.149321</td>\n",
       "      <td id=\"T_3b9f0_row2_col2\" class=\"data row2 col2\" >0.437891</td>\n",
       "      <td id=\"T_3b9f0_row2_col3\" class=\"data row2 col3\" >0.346310</td>\n",
       "      <td id=\"T_3b9f0_row2_col4\" class=\"data row2 col4\" >0.051451</td>\n",
       "      <td id=\"T_3b9f0_row2_col5\" class=\"data row2 col5\" >1.772412</td>\n",
       "      <td id=\"T_3b9f0_row2_col6\" class=\"data row2 col6\" >neutral</td>\n",
       "      <td id=\"T_3b9f0_row2_col7\" class=\"data row2 col7\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b9f0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3b9f0_row3_col0\" class=\"data row3 col0\" >0.009685</td>\n",
       "      <td id=\"T_3b9f0_row3_col1\" class=\"data row3 col1\" >0.063449</td>\n",
       "      <td id=\"T_3b9f0_row3_col2\" class=\"data row3 col2\" >0.309403</td>\n",
       "      <td id=\"T_3b9f0_row3_col3\" class=\"data row3 col3\" >0.478728</td>\n",
       "      <td id=\"T_3b9f0_row3_col4\" class=\"data row3 col4\" >0.138735</td>\n",
       "      <td id=\"T_3b9f0_row3_col5\" class=\"data row3 col5\" >1.744949</td>\n",
       "      <td id=\"T_3b9f0_row3_col6\" class=\"data row3 col6\" >negative</td>\n",
       "      <td id=\"T_3b9f0_row3_col7\" class=\"data row3 col7\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b9f0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3b9f0_row4_col0\" class=\"data row4 col0\" >0.016091</td>\n",
       "      <td id=\"T_3b9f0_row4_col1\" class=\"data row4 col1\" >0.007457</td>\n",
       "      <td id=\"T_3b9f0_row4_col2\" class=\"data row4 col2\" >0.028653</td>\n",
       "      <td id=\"T_3b9f0_row4_col3\" class=\"data row4 col3\" >0.138069</td>\n",
       "      <td id=\"T_3b9f0_row4_col4\" class=\"data row4 col4\" >0.809729</td>\n",
       "      <td id=\"T_3b9f0_row4_col5\" class=\"data row4 col5\" >0.936368</td>\n",
       "      <td id=\"T_3b9f0_row4_col6\" class=\"data row4 col6\" >very negative</td>\n",
       "      <td id=\"T_3b9f0_row4_col7\" class=\"data row4 col7\" >4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5c0351e190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_utils import pipeline_results_to_df, logits_to_df\n",
    "\n",
    "texts = ['perfect!', 'good', 'meh', 'i dont like it', 'i hate it!',]\n",
    "\n",
    "#\n",
    "# Using pipeline\n",
    "#\n",
    "from transformers import pipeline\n",
    "\n",
    "if False:\n",
    "    pl = pipeline('sentiment-analysis', model, tokenizer=tokenizer, top_k=None)\n",
    "    ans = pl(texts)\n",
    "    display(ans)\n",
    "\n",
    "    pipeline_results_to_df(ans, model.config.label2id)\\\n",
    "        .style\\\n",
    "        .text_gradient(subset=['very positive', 'positive', 'neutral', 'very negative', 'negative'], axis=1, cmap='Purples_r')\n",
    "\n",
    "\n",
    "#\n",
    "# Using logits\n",
    "#\n",
    "with torch.no_grad():\n",
    "    logits = model(**tokenizer(texts, padding=True, return_tensors='pt').to(device))['logits']\n",
    "\n",
    "logits_to_df(logits.cpu(), model.config.id2label)\\\n",
    "    .style\\\n",
    "    .text_gradient(subset=['very positive', 'positive', 'neutral', 'very negative', 'negative'], axis=1, cmap='Purples_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Sentiment data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_da5f9_row1653_col8, #T_da5f9_row2607_col8 {\n",
       "  background-color: #f88c51;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_da5f9_row2732_col8, #T_da5f9_row3264_col8 {\n",
       "  background-color: #87cb67;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_da5f9_row4931_col8 {\n",
       "  background-color: #fffebe;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_da5f9\">\n",
       "  <caption>shape: (5239, 10)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_da5f9_level0_col0\" class=\"col_heading level0 col0\" >Comment ID</th>\n",
       "      <th id=\"T_da5f9_level0_col1\" class=\"col_heading level0 col1\" >Trust</th>\n",
       "      <th id=\"T_da5f9_level0_col2\" class=\"col_heading level0 col2\" >Respondent ID</th>\n",
       "      <th id=\"T_da5f9_level0_col3\" class=\"col_heading level0 col3\" >Date</th>\n",
       "      <th id=\"T_da5f9_level0_col4\" class=\"col_heading level0 col4\" >question_type</th>\n",
       "      <th id=\"T_da5f9_level0_col5\" class=\"col_heading level0 col5\" >answer_clean</th>\n",
       "      <th id=\"T_da5f9_level0_col6\" class=\"col_heading level0 col6\" >answer_char_len</th>\n",
       "      <th id=\"T_da5f9_level0_col7\" class=\"col_heading level0 col7\" >answer_word_len</th>\n",
       "      <th id=\"T_da5f9_level0_col8\" class=\"col_heading level0 col8\" >Comment sentiment</th>\n",
       "      <th id=\"T_da5f9_level0_col9\" class=\"col_heading level0 col9\" >sentiment_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_da5f9_level0_row1653\" class=\"row_heading level0 row1653\" >4772</th>\n",
       "      <td id=\"T_da5f9_row1653_col0\" class=\"data row1653 col0\" >NEAS PTS 1957 - Q1</td>\n",
       "      <td id=\"T_da5f9_row1653_col1\" class=\"data row1653 col1\" >NEAS</td>\n",
       "      <td id=\"T_da5f9_row1653_col2\" class=\"data row1653 col2\" >NEAS PTS 1957</td>\n",
       "      <td id=\"T_da5f9_row1653_col3\" class=\"data row1653 col3\" >30/09/2022</td>\n",
       "      <td id=\"T_da5f9_row1653_col4\" class=\"data row1653 col4\" >nonspecific</td>\n",
       "      <td id=\"T_da5f9_row1653_col5\" class=\"data row1653 col5\" >Usually pick up on time but on occasion quite late and all occasion so late my appointment with consultant was put in jeopardy.</td>\n",
       "      <td id=\"T_da5f9_row1653_col6\" class=\"data row1653 col6\" >127</td>\n",
       "      <td id=\"T_da5f9_row1653_col7\" class=\"data row1653 col7\" >23</td>\n",
       "      <td id=\"T_da5f9_row1653_col8\" class=\"data row1653 col8\" >4</td>\n",
       "      <td id=\"T_da5f9_row1653_col9\" class=\"data row1653 col9\" >negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da5f9_level0_row2607\" class=\"row_heading level0 row2607\" >4765</th>\n",
       "      <td id=\"T_da5f9_row2607_col0\" class=\"data row2607 col0\" >NEAS PTS 1936 - Q1</td>\n",
       "      <td id=\"T_da5f9_row2607_col1\" class=\"data row2607 col1\" >NEAS</td>\n",
       "      <td id=\"T_da5f9_row2607_col2\" class=\"data row2607 col2\" >NEAS PTS 1936</td>\n",
       "      <td id=\"T_da5f9_row2607_col3\" class=\"data row2607 col3\" >30/09/2022</td>\n",
       "      <td id=\"T_da5f9_row2607_col4\" class=\"data row2607 col4\" >nonspecific</td>\n",
       "      <td id=\"T_da5f9_row2607_col5\" class=\"data row2607 col5\" >Quicker response to return home.</td>\n",
       "      <td id=\"T_da5f9_row2607_col6\" class=\"data row2607 col6\" >32</td>\n",
       "      <td id=\"T_da5f9_row2607_col7\" class=\"data row2607 col7\" >5</td>\n",
       "      <td id=\"T_da5f9_row2607_col8\" class=\"data row2607 col8\" >4</td>\n",
       "      <td id=\"T_da5f9_row2607_col9\" class=\"data row2607 col9\" >negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da5f9_level0_row2732\" class=\"row_heading level0 row2732\" >3439</th>\n",
       "      <td id=\"T_da5f9_row2732_col0\" class=\"data row2732 col0\" >NEAS PTS 903 - Q2</td>\n",
       "      <td id=\"T_da5f9_row2732_col1\" class=\"data row2732 col1\" >NEAS</td>\n",
       "      <td id=\"T_da5f9_row2732_col2\" class=\"data row2732 col2\" >NEAS PTS 903</td>\n",
       "      <td id=\"T_da5f9_row2732_col3\" class=\"data row2732 col3\" >31/01/2022</td>\n",
       "      <td id=\"T_da5f9_row2732_col4\" class=\"data row2732 col4\" >could_improve</td>\n",
       "      <td id=\"T_da5f9_row2732_col5\" class=\"data row2732 col5\" >As far as I know it was the best. Seeing the other ambulance did not turn up at all that's why you were called you came quick. Many thanks.</td>\n",
       "      <td id=\"T_da5f9_row2732_col6\" class=\"data row2732 col6\" >139</td>\n",
       "      <td id=\"T_da5f9_row2732_col7\" class=\"data row2732 col7\" >29</td>\n",
       "      <td id=\"T_da5f9_row2732_col8\" class=\"data row2732 col8\" >2</td>\n",
       "      <td id=\"T_da5f9_row2732_col9\" class=\"data row2732 col9\" >positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da5f9_level0_row3264\" class=\"row_heading level0 row3264\" >3107</th>\n",
       "      <td id=\"T_da5f9_row3264_col0\" class=\"data row3264 col0\" >NEAS 111 2015 - Q2</td>\n",
       "      <td id=\"T_da5f9_row3264_col1\" class=\"data row3264 col1\" >NEAS</td>\n",
       "      <td id=\"T_da5f9_row3264_col2\" class=\"data row3264 col2\" >NEAS 111 2015</td>\n",
       "      <td id=\"T_da5f9_row3264_col3\" class=\"data row3264 col3\" >10/05/2022</td>\n",
       "      <td id=\"T_da5f9_row3264_col4\" class=\"data row3264 col4\" >could_improve</td>\n",
       "      <td id=\"T_da5f9_row3264_col5\" class=\"data row3264 col5\" >Got a prompt response and appointment within 1 hour</td>\n",
       "      <td id=\"T_da5f9_row3264_col6\" class=\"data row3264 col6\" >51</td>\n",
       "      <td id=\"T_da5f9_row3264_col7\" class=\"data row3264 col7\" >9</td>\n",
       "      <td id=\"T_da5f9_row3264_col8\" class=\"data row3264 col8\" >2</td>\n",
       "      <td id=\"T_da5f9_row3264_col9\" class=\"data row3264 col9\" >positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da5f9_level0_row4931\" class=\"row_heading level0 row4931\" >2738</th>\n",
       "      <td id=\"T_da5f9_row4931_col0\" class=\"data row4931 col0\" >NHFT 1341 - Q2</td>\n",
       "      <td id=\"T_da5f9_row4931_col1\" class=\"data row4931 col1\" >NHFT</td>\n",
       "      <td id=\"T_da5f9_row4931_col2\" class=\"data row4931 col2\" >NHFT 1341</td>\n",
       "      <td id=\"T_da5f9_row4931_col3\" class=\"data row4931 col3\" >14/03/2022</td>\n",
       "      <td id=\"T_da5f9_row4931_col4\" class=\"data row4931 col4\" >what_good</td>\n",
       "      <td id=\"T_da5f9_row4931_col5\" class=\"data row4931 col5\" >Yes.</td>\n",
       "      <td id=\"T_da5f9_row4931_col6\" class=\"data row4931 col6\" >4</td>\n",
       "      <td id=\"T_da5f9_row4931_col7\" class=\"data row4931 col7\" >1</td>\n",
       "      <td id=\"T_da5f9_row4931_col8\" class=\"data row4931 col8\" >3</td>\n",
       "      <td id=\"T_da5f9_row4931_col9\" class=\"data row4931 col9\" >neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5c03476050>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spreadsheet_data_handling import tweak_for_sentiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Could use tweak_generic for MLM/domain-adaptation, but stick to\n",
    "# tweak_for_sentiment to be consistent with previous data splits\n",
    "tweaked_df = (\n",
    "    pd.read_csv('../pxtextmining/datasets/v6framework_230831.csv')\n",
    "    .pipe(tweak_for_sentiment)\n",
    ")\n",
    "\n",
    "tweaked_df, test_df = train_test_split(\n",
    "    tweaked_df, test_size=0.15,\n",
    "    random_state=0,\n",
    "    stratify=tweaked_df['Comment sentiment']\n",
    ")\n",
    "\n",
    "tweaked_df.style\\\n",
    "    .set_caption(f'shape: {tweaked_df.shape}')\\\n",
    "    .background_gradient(subset=['Comment sentiment'], cmap='RdYlGn_r')\\\n",
    "    .hide(subset=tweaked_df.sample(n=len(tweaked_df) - 5).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified train-val split and to `DatasetDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd8319e7c9046ec899dda0cd43bb4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ded30f41f4496d8b0de5dc2d0215cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db86eff68814ab182a80ae3574785df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_type', 'answer_clean', 'sentiment_desc', 'comment_sentiment'],\n",
       "        num_rows: 3929\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_type', 'answer_clean', 'sentiment_desc', 'comment_sentiment'],\n",
       "        num_rows: 1310\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question_type', 'answer_clean', 'sentiment_desc', 'comment_sentiment'],\n",
       "        num_rows: 925\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question_type': ['could_improve', 'could_improve'],\n",
       " 'answer_clean': ['Only that other participants sometimes had background sounds.',\n",
       "  \"In the present circumstances, with fewer staff, and the public's lack of appreciation I felt I had to mention what I thought was an outstanding person who went above and beyond. A gentleman\"],\n",
       " 'sentiment_desc': ['negative', 'very positive'],\n",
       " 'comment_sentiment': [3, 0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "fft_df_trn, fft_df_val = train_test_split(\n",
    "    tweaked_df, test_size=1/4, random_state=0,\n",
    "    stratify=tweaked_df['Comment sentiment']\n",
    ")\n",
    "\n",
    "fft_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(fft_df_trn),\n",
    "    'validation': Dataset.from_pandas(fft_df_val),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "}).remove_columns([\n",
    "    'Comment ID', 'Trust', 'Respondent ID', 'Date',\n",
    "    'answer_char_len', 'answer_word_len', '__index_level_0__',\n",
    "])\n",
    "\n",
    "#Subtract 1 from score to get 0:4 for pos:neg\n",
    "fft_datasets = fft_datasets.map(\n",
    "    lambda batch:\n",
    "    {'comment_sentiment': [score - 1 for score in batch['Comment sentiment']]},\n",
    "    batched=True\n",
    ").remove_columns(['Comment sentiment',])\n",
    "\n",
    "display(fft_datasets)\n",
    "\n",
    "\n",
    "#View sample\n",
    "fft_datasets['train'].shuffle()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than learning new token embeddings for question types, start by leveraging existing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae00863d9a4949d3b6ab4cb8e824a5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f070fc284904d6f996a887ab9161560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15849bd88044559a4e1b68e0fb80b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question_type': ['could_improve'],\n",
       " 'answer_clean': [\"They couldn't have done anything better.\"],\n",
       " 'sentiment_desc': ['very positive'],\n",
       " 'comment_sentiment': [0],\n",
       " 'q_and_a': [\"Question: What could be improved? Answer: They couldn't have done anything better.\"]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_utils import combine_question_answer\n",
    "\n",
    "fft_datasets = fft_datasets.map(combine_question_answer, batched=True)\n",
    "fft_datasets['train'].shuffle()[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbc29c29e044125b8d6ece1558f4d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79746b4197f4eb89bc9717bc15f050c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd471a51230464b91677805ee31df5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fft_tokenized = fft_datasets.map(\n",
    "    lambda batch: tokenizer(batch['q_and_a'], padding=False, truncation=True),\n",
    "    batched=True\n",
    ").remove_columns([\n",
    "    'question_type', 'answer_clean', 'sentiment_desc', 'q_and_a',\n",
    "]).rename_column('comment_sentiment', 'labels')\n",
    "\n",
    "if 'token_type_ids' in fft_tokenized['train']:\n",
    "    fft_tokenized = fft_tokenized.remove_column('token_type_ids')\n",
    "\n",
    "fft_tokenized['train'].shuffle()[:1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune model on FFT data\n",
    "\n",
    "Regular class-balancing helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Class weights [0.5 0.9 1.7 0.9 4.8]\n",
      "19 epochs | 18677 training steps\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Before FFT fine-tuning~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.19      0.30       512\n",
      "           1       0.40      0.65      0.49       300\n",
      "           2       0.20      0.26      0.23       152\n",
      "           3       0.30      0.37      0.33       291\n",
      "           4       0.16      0.40      0.23        55\n",
      "\n",
      "    accuracy                           0.35      1310\n",
      "   macro avg       0.37      0.37      0.32      1310\n",
      "weighted avg       0.49      0.35      0.34      1310\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fed03ce3ce348afacccfaca29f94a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/18677 [00:00<?, ?minibatch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81       512\n",
      "           1       0.59      0.68      0.63       300\n",
      "           2       0.53      0.43      0.48       152\n",
      "           3       0.82      0.75      0.78       291\n",
      "           4       0.58      0.69      0.63        55\n",
      "\n",
      "    accuracy                           0.71      1310\n",
      "   macro avg       0.67      0.67      0.67      1310\n",
      "weighted avg       0.72      0.71      0.71      1310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "\n",
    "#Load weights into clean model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=5)\n",
    "model.load_state_dict(torch.load('finetuned/tinybert-sst5.pickle', weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "#Run on DistilBERT for comparison to approach in pxtextmining\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=5)\n",
    "# model.to(device)\n",
    "\n",
    "#Class weights\n",
    "class_weights = None\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1, 2, 3, 4]), y=fft_tokenized['train'][:]['labels'])\n",
    "\n",
    "if class_weights is not None:\n",
    "    print('[!] Class weights', class_weights.round(1))\n",
    "\n",
    "\n",
    "#Data loaders\n",
    "batch_size = 4\n",
    "\n",
    "dynamic_padding_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    fft_tokenized['train'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=dynamic_padding_collator,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    fft_tokenized['validation'],\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    collate_fn=dynamic_padding_collator,\n",
    ")\n",
    "val_labels = np.concatenate([minibatch['labels'] for minibatch in val_loader])\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    fft_tokenized['test'],\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    collate_fn=dynamic_padding_collator\n",
    ")\n",
    "\n",
    "#Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 19\n",
    "earlystop_at_epoch = 9\n",
    "\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(round(0.1 * num_training_steps)),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_epochs, 'epochs |', num_training_steps, 'training steps')\n",
    "\n",
    "\n",
    "#Score before training\n",
    "preds, loss = compute_metrics(model, val_loader, device)\n",
    "print('Before FFT fine-tuning'.center(85, '~'))\n",
    "print(classification_report(val_labels, preds))\n",
    "\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps), unit='minibatch', desc='training')\n",
    "writer = SummaryWriter(f'runs/fft_ft-bs{batch_size}')\n",
    "\n",
    "#\n",
    "# Training loop\n",
    "#\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for minibatch in train_loader:\n",
    "        minibatch = {k: v.to(device) for k, v in minibatch.items()}\n",
    "        outputs = model(**minibatch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # outputs.loss.backward()\n",
    "        torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(device))(outputs['logits'], minibatch['labels']).backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update()\n",
    "    #/end of epoch\n",
    "\n",
    "    model.eval()\n",
    "    trn_preds, trn_loss = compute_metrics(model, train_loader, device)\n",
    "    val_preds, val_loss = compute_metrics(model, val_loader, device)\n",
    "\n",
    "    writer.add_scalars('loss', {'train': trn_loss, 'val': val_loss}, global_step=epoch + 1)\n",
    "    \n",
    "    clf_report = classification_report(val_labels, val_preds, zero_division=np.nan)\n",
    "    writer.add_text('classification_report', clf_report, global_step=epoch + 1)\n",
    "\n",
    "    if epoch + 1 == earlystop_at_epoch: break\n",
    "\n",
    "torch.save(model.state_dict(), f'finetuned/tinybert-sst5-fft.pickle')\n",
    "\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.80      0.81       512\n",
    "           1       0.59      0.68      0.63       300\n",
    "           2       0.53      0.43      0.48       152\n",
    "           3       0.82      0.75      0.78       291\n",
    "           4       0.58      0.69      0.63        55\n",
    "\n",
    "    accuracy                           0.71      1310\n",
    "   macro avg       0.67      0.67      0.67      1310\n",
    "weighted avg       0.72      0.71      0.71      1310\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold tuning\n",
    "\n",
    "Seems like there's scope to tune it to ~0.7 (for class 4 atleast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f5b10a48790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_logits = torch.row_stack([model(**{k: v.to(device) for k, v in minibatch.items()}).logits for minibatch in val_loader])\n",
    "\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "k = 4\n",
    "PrecisionRecallDisplay.from_predictions(val_labels==k, val_logits.softmax(dim=1).cpu()[:, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81       362\n",
      "           1       0.60      0.69      0.64       212\n",
      "           2       0.39      0.38      0.38       108\n",
      "           3       0.80      0.76      0.78       205\n",
      "           4       0.69      0.53      0.60        38\n",
      "\n",
      "    accuracy                           0.71       925\n",
      "   macro avg       0.66      0.63      0.64       925\n",
      "weighted avg       0.71      0.71      0.71       925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=5)\n",
    "# model.load_state_dict(torch.load('finetuned/distilbert-fft.pickle', weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "test_preds, test_loss = compute_metrics(model, test_loader, device)\n",
    "\n",
    "print(classification_report(\n",
    "    y_true=np.concatenate([minibatch['labels'] for minibatch in test_loader]),\n",
    "    y_pred=test_preds\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tinybert-sst5-fft\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.80      0.81       362\n",
    "           1       0.60      0.69      0.64       212\n",
    "           2       0.39      0.38      0.38       108\n",
    "           3       0.80      0.76      0.78       205\n",
    "           4       0.69      0.53      0.60        38\n",
    "\n",
    "    accuracy                           0.71       925\n",
    "   macro avg       0.66      0.63      0.64       925\n",
    "weighted avg       0.71      0.71      0.71       925\n",
    "\n",
    "\n",
    "fft directly finetuned base distilbert (6 epochs)\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.78      0.81       362\n",
    "           1       0.62      0.67      0.64       212\n",
    "           2       0.38      0.46      0.42       108\n",
    "           3       0.84      0.73      0.78       205\n",
    "           4       0.50      0.68      0.58        38\n",
    "\n",
    "    accuracy                           0.70       925\n",
    "   macro avg       0.64      0.67      0.65       925\n",
    "weighted avg       0.72      0.70      0.71       925\n",
    "\n",
    "\n",
    "copy of pxtextmining/distilbert sentiment score\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "very positive       0.80      0.79      0.80      1746\n",
    "     positive       0.63      0.52      0.57       841\n",
    "      neutral       0.52      0.71      0.60       551\n",
    "     negative       0.79      0.68      0.73       639\n",
    "very negative       0.52      0.64      0.57       166\n",
    "\n",
    "     accuracy                           0.70      3943\n",
    "    macro avg       0.65      0.67      0.65      3943\n",
    " weighted avg       0.71      0.70      0.70      3943\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
