{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks on base TinyBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'huawei-noah/TinyBERT_General_4L_312D'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=312, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.381394 million params\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "checkpoint = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "display(\n",
    "    checkpoint,\n",
    "    model,\n",
    ")\n",
    "\n",
    "print(\n",
    "    sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6,\n",
    "    'million params'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `tokenizer.model_max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length = 512\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2009, 2001, 1037, 3185, 2055, 103, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] it was a movie about [MASK]. [SEP]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 0.0395,  0.7269,  0.2387,  ...,  0.8403,  0.8712,  0.4771],\n",
       "         [-0.0463,  0.1668,  0.1976,  ..., -0.0349,  0.2082,  0.0940],\n",
       "         [-0.0659, -0.1432, -0.1890,  ...,  0.0323,  0.2478,  0.3310],\n",
       "         ...,\n",
       "         [-0.0183, -0.1747, -0.1914,  ...,  0.0553, -0.0635,  0.6340],\n",
       "         [ 0.0674,  1.1103,  0.7440,  ...,  0.3351,  0.0164,  0.2027],\n",
       "         [ 0.0752,  1.0598,  0.7429,  ...,  0.3009,  0.0412,  0.3249]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM head output shape: torch.Size([1, 9, 30522])\n",
      "www https oh previously http\n"
     ]
    }
   ],
   "source": [
    "text = 'It was a movie about [MASK].'\n",
    "\n",
    "display(\n",
    "    tokenizer(text),\n",
    "    tokenizer.decode( tokenizer(text)['input_ids'] ),\n",
    "    model(**tokenizer(text, return_tensors='pt')),\n",
    ")\n",
    "\n",
    "outputs = model(**tokenizer(text, return_tensors='pt'))\n",
    "print('LM head output shape:', outputs.logits.shape)\n",
    "\n",
    "logits = outputs['logits'] #(B, L, vocab)\n",
    "\n",
    "#mask_id = tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map['mask_token'])\n",
    "mask_id = tokenizer.vocab[tokenizer.special_tokens_map['mask_token']]\n",
    "mask_ix = tokenizer(text).input_ids.index(mask_id)\n",
    "\n",
    "print(tokenizer.decode(\n",
    "    logits.detach().numpy()[0, mask_ix].argsort()[::-1][:5]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Fine-tune on SST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SST5 data\n",
    "\n",
    "SST5\n",
    "https://github.com/CS287/HW1/tree/master/data\n",
    "\n",
    "(0: very negative, 1: negative, 2: neutral, 3: positive, 4: very positive)\n",
    "\n",
    "Drop test set aot combining with train|val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 8544\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 1101\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# datasets = load_dataset('glue', 'sst2')\n",
    "# datasets = load_dataset('stanfordnlp/sentiment140', trust_remote_code=True)\n",
    "# datasets = load_dataset('stanfordnlp/sst', trust_remote_code=True)\n",
    "datasets = load_dataset('SetFit/sst5', trust_remote_code=True)\n",
    "datasets.pop('test')\n",
    "display(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap labels around. Want sst 0,1,2,3,4 neg:pos -> fft (4, 3, 2, 1, 0) neg:pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'sst5_labels', 'label_text', 'fft_labels'],\n",
       "        num_rows: 8544\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'sst5_labels', 'label_text', 'fft_labels'],\n",
       "        num_rows: 1101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst5_to_fft = {\n",
    "    0: 4, #v neg\n",
    "    1: 3,\n",
    "    2: 2,\n",
    "    3: 1,\n",
    "    4: 0, #v pos\n",
    "}\n",
    "datasets = datasets.map(\n",
    "    lambda batch: {'fft_labels': [sst5_to_fft[sst_score] for sst_score in batch['label']]},\n",
    "    batched=True\n",
    ").rename_column('label', 'sst5_labels')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['an extraordinarily silly thriller .',\n",
       "  'the wonder of mostly martha is the performance of gedeck , who makes martha enormously endearing .'],\n",
       " 'sst5_labels': [2, 4],\n",
       " 'label_text': ['neutral', 'very positive'],\n",
       " 'fft_labels': [2, 0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'].shuffle()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of labels\n",
    "\n",
    "Class 4 relatively small. Will use class weights to boost scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fft_labels\n",
       "0    15.0\n",
       "1    27.0\n",
       "2    19.0\n",
       "3    26.0\n",
       "4    13.0\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets.set_format('pandas')\n",
    "datasets['train'][:].fft_labels.value_counts(normalize=True).mul(100).sort_index().round().pipe(display)\n",
    "datasets.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8544\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = datasets\\\n",
    "    .map(lambda batch: tokenizer(batch['text'], truncation=True), batched=True)\\\n",
    "    .remove_columns(['token_type_ids', 'text', 'label_text', 'sst5_labels'])\\\n",
    "    .rename_column('fft_labels', 'labels')\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#Swap out the classifiation head for a new 5-class head.\n",
    "# Could alternatively use .from_trained(..., ignore_mismatched_sizes=True),\n",
    "# but seems undocumented as of yet.\n",
    "dbert_base = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "if False:\n",
    "    [new_preclf] = [module for name, module in dbert_base.named_modules() if name == 'pre_classifier']\n",
    "    [new_clf] = [module for name, module in dbert_base.named_modules() if name == 'classifier']\n",
    "\n",
    "    dbert_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'distilbert/distilbert-base-uncased-finetuned-sst-2-english' #, num_labels=5, ignore_mismatched_sizes=True\n",
    "    )\n",
    "    dbert_ft.pre_classifier = deepcopy(new_preclf)\n",
    "    dbert_ft.classifier = deepcopy(new_clf)\n",
    "\n",
    "    del dbert_base, new_preclf, new_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune on SST5, then data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: cuda  GPU name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device is:', device, ' GPU name:', torch.cuda.get_device_name())\n",
    "\n",
    "from model_utils import compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SST5 fine-tuning\n",
    "\n",
    "Class4 better with class balancing but rest v bad\n",
    "\n",
    "If boost class4 and keep others 1, ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Using class weights: [1. 1. 1. 1. 2.]\n",
      "15 epochs | 1005 training steps\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Prior to fine-tuning~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0        nan      0.00      0.00       165\n",
      "           1       0.26      0.99      0.41       279\n",
      "           2        nan      0.00      0.00       229\n",
      "           3        nan      0.00      0.00       289\n",
      "           4       0.30      0.04      0.08       139\n",
      "\n",
      "    accuracy                           0.26      1101\n",
      "   macro avg       0.28      0.21      0.10      1101\n",
      "weighted avg       0.27      0.26      0.11      1101\n",
      "\n",
      "  loss: 1.6086375205544967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b664257b3e4ceab7f26f91d5c17fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train steps:   0%|          | 0/1005 [00:00<?, ?minibatch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch  1) | trn 1.5959 | val 1.5976\n",
      "(epoch  2) | trn 1.4587 | val 1.4654\n",
      "(epoch  3) | trn 1.3501 | val 1.3700\n",
      "(epoch  4) | trn 1.2629 | val 1.3031\n",
      "(epoch  5) | trn 1.2230 | val 1.2800\n",
      "(epoch  6) | trn 1.1959 | val 1.2620\n",
      "(epoch  7) | trn 1.1677 | val 1.2564\n",
      "(epoch  8) | trn 1.1417 | val 1.2451\n",
      "(epoch  9) | trn 1.1177 | val 1.2341\n",
      "(epoch 10) | trn 1.1018 | val 1.2294\n",
      "(epoch 11) | trn 1.0974 | val 1.2297\n",
      "(epoch 12) | trn 1.0880 | val 1.2321\n",
      "(epoch 13) | trn 1.0822 | val 1.2281\n",
      "(epoch 14) | trn 1.0834 | val 1.2335\n",
      "(epoch 15) | trn 1.0787 | val 1.2298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.51      0.53       165\n",
      "           1       0.46      0.61      0.53       279\n",
      "           2       0.36      0.14      0.20       229\n",
      "           3       0.45      0.38      0.41       289\n",
      "           4       0.39      0.70      0.50       139\n",
      "\n",
      "    accuracy                           0.45      1101\n",
      "   macro avg       0.44      0.47      0.43      1101\n",
      "weighted avg       0.44      0.45      0.43      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=5).to(device)\n",
    "model.config.id2label = {clas: label for clas, label in enumerate(['very positive', 'positive', 'neutral', 'negative', 'very negative'])}\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "#Class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1, 2, 3, 4]), y=tokenized['train'][:]['labels'])\n",
    "class_weights = None\n",
    "class_weights = np.array([1., 1., 1., 1., 2.])\n",
    "\n",
    "if class_weights is not None:\n",
    "    print('[!] Using class weights:', class_weights.round(1))\n",
    "\n",
    "#Data loaders\n",
    "batch_size = 128\n",
    "\n",
    "dynamic_padding_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized['train'], batch_size=batch_size, shuffle=True,\n",
    "    pin_memory=True, collate_fn=dynamic_padding_collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized['validation'], batch_size=batch_size,\n",
    "    pin_memory=True, collate_fn=dynamic_padding_collator\n",
    ")\n",
    "\n",
    "val_labels = np.concatenate([minibatch['labels'] for minibatch in val_loader])\n",
    "\n",
    "\n",
    "#LR scheduler\n",
    "num_epochs = 15\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "print(num_epochs, 'epochs |', num_training_steps, 'training steps')\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear', optimizer,\n",
    "    num_warmup_steps=int(round(0.1 * num_training_steps)),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "#Score before training\n",
    "preds, loss = compute_metrics(model, val_loader, device)\n",
    "print('Prior to fine-tuning'.center(85, '~'))\n",
    "print(classification_report(y_true=val_labels, y_pred=preds, zero_division=np.nan))\n",
    "print('  loss:', loss)\n",
    "\n",
    "\n",
    "#\n",
    "# Training loop\n",
    "#\n",
    "writer = SummaryWriter(log_dir='runs/tinybert_ft_sst5/' + f'bs{batch_size}-cw4')\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps), unit='minibatch', desc='train steps')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for minibatch in train_loader:\n",
    "        minibatch = {k: v.to(device) for k, v in minibatch.items()}\n",
    "        outputs = model(**minibatch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs.loss.backward()\n",
    "        torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device).float())(outputs['logits'], minibatch['labels']).backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update()\n",
    "    #/end of epoch\n",
    "\n",
    "    model.eval()\n",
    "    trn_preds, trn_loss = compute_metrics(model, train_loader, device)\n",
    "    val_preds, val_loss = compute_metrics(model, val_loader, device)\n",
    "\n",
    "    print(f'(epoch {epoch + 1:2d}) | trn {trn_loss:6.4f} | val {val_loss:6.4f}')\n",
    "\n",
    "    writer.add_scalars('loss', {'trn': trn_loss, 'val': val_loss}, global_step=epoch + 1)\n",
    "    \n",
    "    #Class k metrics\n",
    "    k = 4\n",
    "    class_k_true = val_labels==k\n",
    "    class_k_pred = val_preds==k\n",
    "    writer.add_scalars(\n",
    "        f'class {k} val metrics',\n",
    "        {'recall': recall_score(class_k_true, class_k_pred),\n",
    "         'f1': f1_score(class_k_true, class_k_pred)\n",
    "         },\n",
    "         global_step=epoch + 1\n",
    "    )\n",
    "\n",
    "    clf_report = classification_report(val_labels, val_preds, zero_division=np.nan)\n",
    "    writer.add_text('classification_report', clf_report, global_step=epoch + 1)\n",
    "torch.save(model.state_dict(), f'finetuned/tinybert-sst5.pickle')\n",
    "\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8007e_row0_col0, #T_8007e_row1_col1, #T_8007e_row2_col3, #T_8007e_row3_col3, #T_8007e_row4_col4 {\n",
       "  color: #fcfbfd;\n",
       "}\n",
       "#T_8007e_row0_col1 {\n",
       "  color: #f5f4f9;\n",
       "}\n",
       "#T_8007e_row0_col2 {\n",
       "  color: #52238d;\n",
       "}\n",
       "#T_8007e_row0_col3 {\n",
       "  color: #41047f;\n",
       "}\n",
       "#T_8007e_row0_col4, #T_8007e_row1_col4, #T_8007e_row2_col0, #T_8007e_row3_col0, #T_8007e_row4_col1 {\n",
       "  color: #3f007d;\n",
       "}\n",
       "#T_8007e_row1_col0 {\n",
       "  color: #a4a1cc;\n",
       "}\n",
       "#T_8007e_row1_col2 {\n",
       "  color: #5d3897;\n",
       "}\n",
       "#T_8007e_row1_col3 {\n",
       "  color: #450b82;\n",
       "}\n",
       "#T_8007e_row2_col1, #T_8007e_row2_col4 {\n",
       "  color: #7261ab;\n",
       "}\n",
       "#T_8007e_row2_col2 {\n",
       "  color: #eae8f2;\n",
       "}\n",
       "#T_8007e_row3_col1 {\n",
       "  color: #4a1486;\n",
       "}\n",
       "#T_8007e_row3_col2 {\n",
       "  color: #8e8bc1;\n",
       "}\n",
       "#T_8007e_row3_col4 {\n",
       "  color: #cbcbe3;\n",
       "}\n",
       "#T_8007e_row4_col0 {\n",
       "  color: #420680;\n",
       "}\n",
       "#T_8007e_row4_col2 {\n",
       "  color: #4a1587;\n",
       "}\n",
       "#T_8007e_row4_col3 {\n",
       "  color: #6e58a7;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8007e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8007e_level0_col0\" class=\"col_heading level0 col0\" >very positive</th>\n",
       "      <th id=\"T_8007e_level0_col1\" class=\"col_heading level0 col1\" >positive</th>\n",
       "      <th id=\"T_8007e_level0_col2\" class=\"col_heading level0 col2\" >neutral</th>\n",
       "      <th id=\"T_8007e_level0_col3\" class=\"col_heading level0 col3\" >negative</th>\n",
       "      <th id=\"T_8007e_level0_col4\" class=\"col_heading level0 col4\" >very negative</th>\n",
       "      <th id=\"T_8007e_level0_col5\" class=\"col_heading level0 col5\" >entropy</th>\n",
       "      <th id=\"T_8007e_level0_col6\" class=\"col_heading level0 col6\" >predicted_label</th>\n",
       "      <th id=\"T_8007e_level0_col7\" class=\"col_heading level0 col7\" >predicted_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8007e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8007e_row0_col0\" class=\"data row0 col0\" >0.461487</td>\n",
       "      <td id=\"T_8007e_row0_col1\" class=\"data row0 col1\" >0.432268</td>\n",
       "      <td id=\"T_8007e_row0_col2\" class=\"data row0 col2\" >0.067329</td>\n",
       "      <td id=\"T_8007e_row0_col3\" class=\"data row0 col3\" >0.022822</td>\n",
       "      <td id=\"T_8007e_row0_col4\" class=\"data row0 col4\" >0.016094</td>\n",
       "      <td id=\"T_8007e_row0_col5\" class=\"data row0 col5\" >1.520321</td>\n",
       "      <td id=\"T_8007e_row0_col6\" class=\"data row0 col6\" >very positive</td>\n",
       "      <td id=\"T_8007e_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8007e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8007e_row1_col0\" class=\"data row1 col0\" >0.294151</td>\n",
       "      <td id=\"T_8007e_row1_col1\" class=\"data row1 col1\" >0.549641</td>\n",
       "      <td id=\"T_8007e_row1_col2\" class=\"data row1 col2\" >0.108834</td>\n",
       "      <td id=\"T_8007e_row1_col3\" class=\"data row1 col3\" >0.034008</td>\n",
       "      <td id=\"T_8007e_row1_col4\" class=\"data row1 col4\" >0.013366</td>\n",
       "      <td id=\"T_8007e_row1_col5\" class=\"data row1 col5\" >1.591210</td>\n",
       "      <td id=\"T_8007e_row1_col6\" class=\"data row1 col6\" >positive</td>\n",
       "      <td id=\"T_8007e_row1_col7\" class=\"data row1 col7\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8007e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_8007e_row2_col0\" class=\"data row2 col0\" >0.032294</td>\n",
       "      <td id=\"T_8007e_row2_col1\" class=\"data row2 col1\" >0.133827</td>\n",
       "      <td id=\"T_8007e_row2_col2\" class=\"data row2 col2\" >0.322659</td>\n",
       "      <td id=\"T_8007e_row2_col3\" class=\"data row2 col3\" >0.377234</td>\n",
       "      <td id=\"T_8007e_row2_col4\" class=\"data row2 col4\" >0.133986</td>\n",
       "      <td id=\"T_8007e_row2_col5\" class=\"data row2 col5\" >1.993906</td>\n",
       "      <td id=\"T_8007e_row2_col6\" class=\"data row2 col6\" >negative</td>\n",
       "      <td id=\"T_8007e_row2_col7\" class=\"data row2 col7\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8007e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_8007e_row3_col0\" class=\"data row3 col0\" >0.016969</td>\n",
       "      <td id=\"T_8007e_row3_col1\" class=\"data row3 col1\" >0.043244</td>\n",
       "      <td id=\"T_8007e_row3_col2\" class=\"data row3 col2\" >0.198782</td>\n",
       "      <td id=\"T_8007e_row3_col3\" class=\"data row3 col3\" >0.436146</td>\n",
       "      <td id=\"T_8007e_row3_col4\" class=\"data row3 col4\" >0.304859</td>\n",
       "      <td id=\"T_8007e_row3_col5\" class=\"data row3 col5\" >1.803636</td>\n",
       "      <td id=\"T_8007e_row3_col6\" class=\"data row3 col6\" >negative</td>\n",
       "      <td id=\"T_8007e_row3_col7\" class=\"data row3 col7\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8007e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_8007e_row4_col0\" class=\"data row4 col0\" >0.035070</td>\n",
       "      <td id=\"T_8007e_row4_col1\" class=\"data row4 col1\" >0.020927</td>\n",
       "      <td id=\"T_8007e_row4_col2\" class=\"data row4 col2\" >0.065357</td>\n",
       "      <td id=\"T_8007e_row4_col3\" class=\"data row4 col3\" >0.199918</td>\n",
       "      <td id=\"T_8007e_row4_col4\" class=\"data row4 col4\" >0.678729</td>\n",
       "      <td id=\"T_8007e_row4_col5\" class=\"data row4 col5\" >1.387254</td>\n",
       "      <td id=\"T_8007e_row4_col6\" class=\"data row4 col6\" >very negative</td>\n",
       "      <td id=\"T_8007e_row4_col7\" class=\"data row4 col7\" >4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2bb037d850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_utils import pipeline_results_to_df, logits_to_df\n",
    "\n",
    "texts = ['perfect!', 'good', 'meh', 'i dont like it', 'i hate it!',]\n",
    "\n",
    "#\n",
    "# Using pipeline\n",
    "#\n",
    "from transformers import pipeline\n",
    "\n",
    "if False:\n",
    "    pl = pipeline('sentiment-analysis', model, tokenizer=tokenizer, top_k=None)\n",
    "    ans = pl(texts)\n",
    "    display(ans)\n",
    "\n",
    "    pipeline_results_to_df(ans, model.config.label2id)\\\n",
    "        .style\\\n",
    "        .text_gradient(subset=['very positive', 'positive', 'neutral', 'very negative', 'negative'], axis=1, cmap='Purples_r')\n",
    "\n",
    "\n",
    "#\n",
    "# Using logits\n",
    "#\n",
    "with torch.no_grad():\n",
    "    logits = model(**tokenizer(texts, padding=True, return_tensors='pt').to(device))['logits']\n",
    "\n",
    "logits_to_df(logits.cpu(), model.config.id2label)\\\n",
    "    .style\\\n",
    "    .text_gradient(subset=['very positive', 'positive', 'neutral', 'very negative', 'negative'], axis=1, cmap='Purples_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Sentiment data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3c273_row1653_col8, #T_3c273_row2607_col8 {\n",
       "  background-color: #f88c51;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c273_row2732_col8, #T_3c273_row3264_col8 {\n",
       "  background-color: #87cb67;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c273_row4931_col8 {\n",
       "  background-color: #fffebe;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3c273\">\n",
       "  <caption>shape: (5239, 10)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3c273_level0_col0\" class=\"col_heading level0 col0\" >Comment ID</th>\n",
       "      <th id=\"T_3c273_level0_col1\" class=\"col_heading level0 col1\" >Trust</th>\n",
       "      <th id=\"T_3c273_level0_col2\" class=\"col_heading level0 col2\" >Respondent ID</th>\n",
       "      <th id=\"T_3c273_level0_col3\" class=\"col_heading level0 col3\" >Date</th>\n",
       "      <th id=\"T_3c273_level0_col4\" class=\"col_heading level0 col4\" >question_type</th>\n",
       "      <th id=\"T_3c273_level0_col5\" class=\"col_heading level0 col5\" >answer_clean</th>\n",
       "      <th id=\"T_3c273_level0_col6\" class=\"col_heading level0 col6\" >answer_char_len</th>\n",
       "      <th id=\"T_3c273_level0_col7\" class=\"col_heading level0 col7\" >answer_word_len</th>\n",
       "      <th id=\"T_3c273_level0_col8\" class=\"col_heading level0 col8\" >Comment sentiment</th>\n",
       "      <th id=\"T_3c273_level0_col9\" class=\"col_heading level0 col9\" >sentiment_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3c273_level0_row1653\" class=\"row_heading level0 row1653\" >4772</th>\n",
       "      <td id=\"T_3c273_row1653_col0\" class=\"data row1653 col0\" >NEAS PTS 1957 - Q1</td>\n",
       "      <td id=\"T_3c273_row1653_col1\" class=\"data row1653 col1\" >NEAS</td>\n",
       "      <td id=\"T_3c273_row1653_col2\" class=\"data row1653 col2\" >NEAS PTS 1957</td>\n",
       "      <td id=\"T_3c273_row1653_col3\" class=\"data row1653 col3\" >30/09/2022</td>\n",
       "      <td id=\"T_3c273_row1653_col4\" class=\"data row1653 col4\" >nonspecific</td>\n",
       "      <td id=\"T_3c273_row1653_col5\" class=\"data row1653 col5\" >Usually pick up on time but on occasion quite late and all occasion so late my appointment with consultant was put in jeopardy.</td>\n",
       "      <td id=\"T_3c273_row1653_col6\" class=\"data row1653 col6\" >127</td>\n",
       "      <td id=\"T_3c273_row1653_col7\" class=\"data row1653 col7\" >23</td>\n",
       "      <td id=\"T_3c273_row1653_col8\" class=\"data row1653 col8\" >4</td>\n",
       "      <td id=\"T_3c273_row1653_col9\" class=\"data row1653 col9\" >negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c273_level0_row2607\" class=\"row_heading level0 row2607\" >4765</th>\n",
       "      <td id=\"T_3c273_row2607_col0\" class=\"data row2607 col0\" >NEAS PTS 1936 - Q1</td>\n",
       "      <td id=\"T_3c273_row2607_col1\" class=\"data row2607 col1\" >NEAS</td>\n",
       "      <td id=\"T_3c273_row2607_col2\" class=\"data row2607 col2\" >NEAS PTS 1936</td>\n",
       "      <td id=\"T_3c273_row2607_col3\" class=\"data row2607 col3\" >30/09/2022</td>\n",
       "      <td id=\"T_3c273_row2607_col4\" class=\"data row2607 col4\" >nonspecific</td>\n",
       "      <td id=\"T_3c273_row2607_col5\" class=\"data row2607 col5\" >Quicker response to return home.</td>\n",
       "      <td id=\"T_3c273_row2607_col6\" class=\"data row2607 col6\" >32</td>\n",
       "      <td id=\"T_3c273_row2607_col7\" class=\"data row2607 col7\" >5</td>\n",
       "      <td id=\"T_3c273_row2607_col8\" class=\"data row2607 col8\" >4</td>\n",
       "      <td id=\"T_3c273_row2607_col9\" class=\"data row2607 col9\" >negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c273_level0_row2732\" class=\"row_heading level0 row2732\" >3439</th>\n",
       "      <td id=\"T_3c273_row2732_col0\" class=\"data row2732 col0\" >NEAS PTS 903 - Q2</td>\n",
       "      <td id=\"T_3c273_row2732_col1\" class=\"data row2732 col1\" >NEAS</td>\n",
       "      <td id=\"T_3c273_row2732_col2\" class=\"data row2732 col2\" >NEAS PTS 903</td>\n",
       "      <td id=\"T_3c273_row2732_col3\" class=\"data row2732 col3\" >31/01/2022</td>\n",
       "      <td id=\"T_3c273_row2732_col4\" class=\"data row2732 col4\" >could_improve</td>\n",
       "      <td id=\"T_3c273_row2732_col5\" class=\"data row2732 col5\" >As far as I know it was the best. Seeing the other ambulance did not turn up at all that's why you were called you came quick. Many thanks.</td>\n",
       "      <td id=\"T_3c273_row2732_col6\" class=\"data row2732 col6\" >139</td>\n",
       "      <td id=\"T_3c273_row2732_col7\" class=\"data row2732 col7\" >29</td>\n",
       "      <td id=\"T_3c273_row2732_col8\" class=\"data row2732 col8\" >2</td>\n",
       "      <td id=\"T_3c273_row2732_col9\" class=\"data row2732 col9\" >positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c273_level0_row3264\" class=\"row_heading level0 row3264\" >3107</th>\n",
       "      <td id=\"T_3c273_row3264_col0\" class=\"data row3264 col0\" >NEAS 111 2015 - Q2</td>\n",
       "      <td id=\"T_3c273_row3264_col1\" class=\"data row3264 col1\" >NEAS</td>\n",
       "      <td id=\"T_3c273_row3264_col2\" class=\"data row3264 col2\" >NEAS 111 2015</td>\n",
       "      <td id=\"T_3c273_row3264_col3\" class=\"data row3264 col3\" >10/05/2022</td>\n",
       "      <td id=\"T_3c273_row3264_col4\" class=\"data row3264 col4\" >could_improve</td>\n",
       "      <td id=\"T_3c273_row3264_col5\" class=\"data row3264 col5\" >Got a prompt response and appointment within 1 hour</td>\n",
       "      <td id=\"T_3c273_row3264_col6\" class=\"data row3264 col6\" >51</td>\n",
       "      <td id=\"T_3c273_row3264_col7\" class=\"data row3264 col7\" >9</td>\n",
       "      <td id=\"T_3c273_row3264_col8\" class=\"data row3264 col8\" >2</td>\n",
       "      <td id=\"T_3c273_row3264_col9\" class=\"data row3264 col9\" >positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c273_level0_row4931\" class=\"row_heading level0 row4931\" >2738</th>\n",
       "      <td id=\"T_3c273_row4931_col0\" class=\"data row4931 col0\" >NHFT 1341 - Q2</td>\n",
       "      <td id=\"T_3c273_row4931_col1\" class=\"data row4931 col1\" >NHFT</td>\n",
       "      <td id=\"T_3c273_row4931_col2\" class=\"data row4931 col2\" >NHFT 1341</td>\n",
       "      <td id=\"T_3c273_row4931_col3\" class=\"data row4931 col3\" >14/03/2022</td>\n",
       "      <td id=\"T_3c273_row4931_col4\" class=\"data row4931 col4\" >what_good</td>\n",
       "      <td id=\"T_3c273_row4931_col5\" class=\"data row4931 col5\" >Yes.</td>\n",
       "      <td id=\"T_3c273_row4931_col6\" class=\"data row4931 col6\" >4</td>\n",
       "      <td id=\"T_3c273_row4931_col7\" class=\"data row4931 col7\" >1</td>\n",
       "      <td id=\"T_3c273_row4931_col8\" class=\"data row4931 col8\" >3</td>\n",
       "      <td id=\"T_3c273_row4931_col9\" class=\"data row4931 col9\" >neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2bb17f3b90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spreadsheet_data_handling import tweak_for_sentiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Could use tweak_generic for MLM/domain-adaptation, but stick to\n",
    "# tweak_for_sentiment to be consistent with previous data splits\n",
    "tweaked_df = (\n",
    "    pd.read_csv('../pxtextmining/datasets/v6framework_230831.csv')\n",
    "    .pipe(tweak_for_sentiment)\n",
    ")\n",
    "\n",
    "tweaked_df, test_df = train_test_split(\n",
    "    tweaked_df, test_size=0.15,\n",
    "    random_state=0,\n",
    "    stratify=tweaked_df['Comment sentiment']\n",
    ")\n",
    "\n",
    "tweaked_df.style\\\n",
    "    .set_caption(f'shape: {tweaked_df.shape}')\\\n",
    "    .background_gradient(subset=['Comment sentiment'], cmap='RdYlGn_r')\\\n",
    "    .hide(subset=tweaked_df.sample(n=len(tweaked_df) - 5).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified train-val split and to `DatasetDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d644cdb0fc4f4ed5bd4ea19386204376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683de6ef338c4f9894851df143643293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581b7c73a3e54a198fc0e24e3ebeee69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_type', 'answer_clean', 'sentiment_desc', 'comment_sentiment'],\n",
       "        num_rows: 3929\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_type', 'answer_clean', 'sentiment_desc', 'comment_sentiment'],\n",
       "        num_rows: 1310\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question_type', 'answer_clean', 'sentiment_desc', 'comment_sentiment'],\n",
       "        num_rows: 925\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question_type': ['could_improve', 'could_improve'],\n",
       " 'answer_clean': ['Only that other participants sometimes had background sounds.',\n",
       "  \"In the present circumstances, with fewer staff, and the public's lack of appreciation I felt I had to mention what I thought was an outstanding person who went above and beyond. A gentleman\"],\n",
       " 'sentiment_desc': ['negative', 'very positive'],\n",
       " 'comment_sentiment': [3, 0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "fft_df_trn, fft_df_val = train_test_split(\n",
    "    tweaked_df, test_size=1/4, random_state=0,\n",
    "    stratify=tweaked_df['Comment sentiment']\n",
    ")\n",
    "\n",
    "fft_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(fft_df_trn),\n",
    "    'validation': Dataset.from_pandas(fft_df_val),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "}).remove_columns([\n",
    "    'Comment ID', 'Trust', 'Respondent ID', 'Date',\n",
    "    'answer_char_len', 'answer_word_len', '__index_level_0__',\n",
    "])\n",
    "\n",
    "#Subtract 1 from score to get 0:4 for pos:neg\n",
    "fft_datasets = fft_datasets.map(\n",
    "    lambda batch:\n",
    "    {'comment_sentiment': [score - 1 for score in batch['Comment sentiment']]},\n",
    "    batched=True\n",
    ").remove_columns(['Comment sentiment',])\n",
    "\n",
    "display(fft_datasets)\n",
    "\n",
    "\n",
    "#View sample\n",
    "fft_datasets['train'].shuffle()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than learning new token embeddings for question types, start by leveraging existing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493d8bd101e54d6a91a440b7cde42526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb2ec4c3fb64471b8f4496d9fa93043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2af365f7b34ebabbe69aa9a9582017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question_type': ['could_improve'],\n",
       " 'answer_clean': [\"They couldn't have done anything better.\"],\n",
       " 'sentiment_desc': ['very positive'],\n",
       " 'comment_sentiment': [0],\n",
       " 'q_and_a': [\"Question: What could be improved? Answer: They couldn't have done anything better.\"]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_utils import combine_question_answer\n",
    "\n",
    "fft_datasets = fft_datasets.map(combine_question_answer, batched=True)\n",
    "fft_datasets['train'].shuffle()[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b341c7ceeb874e14a0541dd65f722d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd69f2fc1d141dca550a865da543a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308635f2fd5c4781a80b9d941cf26189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fft_tokenized = fft_datasets.map(\n",
    "    lambda batch: tokenizer(batch['q_and_a'], padding=False, truncation=True),\n",
    "    batched=True\n",
    ").remove_columns([\n",
    "    'question_type', 'answer_clean', 'sentiment_desc', 'q_and_a',\n",
    "    'token_type_ids',\n",
    "]).rename_column('comment_sentiment', 'labels')\n",
    "\n",
    "fft_tokenized['train'].shuffle()[:1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune model on FFT data\n",
    "\n",
    "Regular class-balancing helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Class weights [0.5 0.9 1.7 0.9 4.8]\n",
      "19 epochs | 2337 training steps\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Before FFT fine-tuning~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.17      0.29       512\n",
      "           1       0.39      0.72      0.51       300\n",
      "           2       0.22      0.24      0.23       152\n",
      "           3       0.45      0.47      0.46       291\n",
      "           4       0.12      0.38      0.18        55\n",
      "\n",
      "    accuracy                           0.38      1310\n",
      "   macro avg       0.40      0.40      0.33      1310\n",
      "weighted avg       0.54      0.38      0.37      1310\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0236ee1d61414cb6fa9ea854cc568e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/2337 [00:00<?, ?minibatch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       512\n",
      "           1       0.60      0.58      0.59       300\n",
      "           2       0.42      0.45      0.44       152\n",
      "           3       0.79      0.65      0.71       291\n",
      "           4       0.44      0.73      0.55        55\n",
      "\n",
      "    accuracy                           0.68      1310\n",
      "   macro avg       0.61      0.64      0.62      1310\n",
      "weighted avg       0.69      0.68      0.68      1310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "\n",
    "#Load weights into clean model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=5)\n",
    "model.load_state_dict(torch.load('finetuned/tinybert-sst5.pickle', weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "#Run on DistilBERT for comparison to approach in pxtextmining\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=5)\n",
    "# model.to(device)\n",
    "\n",
    "#Class weights\n",
    "class_weights = None\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1, 2, 3, 4]), y=fft_tokenized['train'][:]['labels'])\n",
    "\n",
    "if class_weights is not None:\n",
    "    print('[!] Class weights', class_weights.round(1))\n",
    "\n",
    "\n",
    "#Data loaders\n",
    "batch_size = 32\n",
    "\n",
    "dynamic_padding_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    fft_tokenized['train'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=dynamic_padding_collator,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    fft_tokenized['validation'],\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    collate_fn=dynamic_padding_collator,\n",
    ")\n",
    "val_labels = np.concatenate([minibatch['labels'] for minibatch in val_loader])\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    fft_tokenized['test'],\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    collate_fn=dynamic_padding_collator\n",
    ")\n",
    "\n",
    "#Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 19\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(round(0.1 * num_training_steps)),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_epochs, 'epochs |', num_training_steps, 'training steps')\n",
    "\n",
    "\n",
    "#Score before training\n",
    "preds, loss = compute_metrics(model, val_loader, device)\n",
    "print('Before FFT fine-tuning'.center(85, '~'))\n",
    "print(classification_report(val_labels, preds))\n",
    "\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps), unit='minibatch', desc='training')\n",
    "writer = SummaryWriter(f'runs/fft_ft-bs{batch_size}')\n",
    "\n",
    "#\n",
    "# Training loop\n",
    "#\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for minibatch in train_loader:\n",
    "        minibatch = {k: v.to(device) for k, v in minibatch.items()}\n",
    "        outputs = model(**minibatch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # outputs.loss.backward()\n",
    "        torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(device))(outputs['logits'], minibatch['labels']).backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update()\n",
    "    #/end of epoch\n",
    "\n",
    "    model.eval()\n",
    "    trn_preds, trn_loss = compute_metrics(model, train_loader, device)\n",
    "    val_preds, val_loss = compute_metrics(model, val_loader, device)\n",
    "\n",
    "    writer.add_scalars('loss', {'train': trn_loss, 'val': val_loss}, global_step=epoch + 1)\n",
    "    \n",
    "    clf_report = classification_report(val_labels, val_preds, zero_division=np.nan)\n",
    "    writer.add_text('classification_report', clf_report, global_step=epoch + 1)\n",
    "torch.save(model.state_dict(), f'finetuned/tinybert-sst5-fft.pickle')\n",
    "\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold tuning\n",
    "\n",
    "Large drop in prec for higher recall (class 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f2b6bf05ad0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_logits = torch.row_stack([model(**{k: v.to(device) for k, v in minibatch.items()}).logits for minibatch in val_loader])\n",
    "\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "k = 4\n",
    "PrecisionRecallDisplay.from_predictions(val_labels==k, val_logits.softmax(dim=1).cpu()[:, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       362\n",
      "           1       0.61      0.58      0.60       212\n",
      "           2       0.35      0.48      0.40       108\n",
      "           3       0.80      0.69      0.74       205\n",
      "           4       0.54      0.66      0.60        38\n",
      "\n",
      "    accuracy                           0.68       925\n",
      "   macro avg       0.62      0.64      0.63       925\n",
      "weighted avg       0.70      0.68      0.69       925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=5)\n",
    "# model.load_state_dict(torch.load('finetuned/distilbert-fft.pickle', weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "test_preds, test_loss = compute_metrics(model, test_loader, device)\n",
    "\n",
    "print(classification_report(\n",
    "    y_true=np.concatenate([minibatch['labels'] for minibatch in test_loader]),\n",
    "    y_pred=test_preds\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntinybert-sst5-fft (19 epochs)\\n              precision    recall  f1-score   support\\n\\n           0       0.81      0.79      0.80       362\\n           1       0.61      0.58      0.60       212\\n           2       0.35      0.48      0.40       108\\n           3       0.80      0.69      0.74       205\\n           4       0.54      0.66      0.60        38\\n\\n    accuracy                           0.68       925\\n   macro avg       0.62      0.64      0.63       925\\nweighted avg       0.70      0.68      0.69       925\\n\\n\\nfft directly finetuned distilbert (6 epochs)\\n              precision    recall  f1-score   support\\n\\n           0       0.84      0.78      0.81       362\\n           1       0.62      0.67      0.64       212\\n           2       0.38      0.46      0.42       108\\n           3       0.84      0.73      0.78       205\\n           4       0.50      0.68      0.58        38\\n\\n    accuracy                           0.70       925\\n   macro avg       0.64      0.67      0.65       925\\nweighted avg       0.72      0.70      0.71       925\\n\\n\\ncopy of pxtextmining/distilbert sentiment score\\n               precision    recall  f1-score   support\\n\\nvery positive       0.80      0.79      0.80      1746\\n     positive       0.63      0.52      0.57       841\\n      neutral       0.52      0.71      0.60       551\\n     negative       0.79      0.68      0.73       639\\nvery negative       0.52      0.64      0.57       166\\n\\n     accuracy                           0.70      3943\\n    macro avg       0.65      0.67      0.65      3943\\n weighted avg       0.71      0.70      0.70      3943\\n '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tinybert-sst5-fft (19 epochs)\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.79      0.80       362\n",
    "           1       0.61      0.58      0.60       212\n",
    "           2       0.35      0.48      0.40       108\n",
    "           3       0.80      0.69      0.74       205\n",
    "           4       0.54      0.66      0.60        38\n",
    "\n",
    "    accuracy                           0.68       925\n",
    "   macro avg       0.62      0.64      0.63       925\n",
    "weighted avg       0.70      0.68      0.69       925\n",
    "\n",
    "\n",
    "fft directly finetuned base distilbert (6 epochs)\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.78      0.81       362\n",
    "           1       0.62      0.67      0.64       212\n",
    "           2       0.38      0.46      0.42       108\n",
    "           3       0.84      0.73      0.78       205\n",
    "           4       0.50      0.68      0.58        38\n",
    "\n",
    "    accuracy                           0.70       925\n",
    "   macro avg       0.64      0.67      0.65       925\n",
    "weighted avg       0.72      0.70      0.71       925\n",
    "\n",
    "\n",
    "copy of pxtextmining/distilbert sentiment score\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "very positive       0.80      0.79      0.80      1746\n",
    "     positive       0.63      0.52      0.57       841\n",
    "      neutral       0.52      0.71      0.60       551\n",
    "     negative       0.79      0.68      0.73       639\n",
    "very negative       0.52      0.64      0.57       166\n",
    "\n",
    "     accuracy                           0.70      3943\n",
    "    macro avg       0.65      0.67      0.65      3943\n",
    " weighted avg       0.71      0.70      0.70      3943\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
